# Engines register themselves via the ``herbarium.engines`` entry-point group.
# Third-party packages can expose engines with:
# [project.entry-points."herbarium.engines"]
# name = "pkg.module"
# ``preferred_engine`` must correspond to a registered engine name.
[ocr]
# Apple Vision first (95% accuracy), comprehensive cloud API fallbacks
preferred_engine = "vision"
# Prioritized engine order based on research findings and cost-effectiveness:
# 1. Apple Vision (95% accuracy, $0 cost, macOS only)
# 2. Budget APIs: Google Vision, Azure Vision ($1-1.50/1000)
# 3. Premium APIs: Claude, GPT-4o, Gemini ($2.50-15/1000)
# 4. Ultra-premium: GPT-4 Vision ($50/1000)
enabled_engines = ["vision", "google", "azure", "textract", "gemini", "claude", "gpt4o", "gpt"]
confidence_threshold = 0.80
# Comprehensive cloud provider coverage
require_api_fallback_on_windows = true
# Accepts ISO 639-1 or ISO 639-2 codes; engines normalize as needed.
langs = ["eng", "fra", "lat"]

[gpt]
# GPT-4 Vision (ultra-premium fallback)
fallback_threshold = 0.95
dry_run = false
model = "gpt-4-vision-preview"
prompt_dir = "prompts"
cost_per_1000 = 50.0

[gpt4o]
# GPT-4o Vision (faster, cheaper than GPT-4)
fallback_threshold = 0.85
model = "gpt-4o"
prompt_dir = "prompts"
cost_per_1000 = 2.5

[claude]
# Claude Vision API for high accuracy herbarium processing
model = "claude-3-5-sonnet-20241022"
fallback_threshold = 0.80
botanical_context = true
cost_per_1000 = 15.0

[google]
# Google Vision API - budget primary choice
credentials_path = ".google-credentials.json"
fallback_threshold = 0.70
cost_per_1000 = 1.5

[azure]
# Microsoft Azure Computer Vision - Windows-optimized
endpoint = "https://your-region.cognitiveservices.azure.com/"
subscription_key_env = "AZURE_COMPUTER_VISION_SUBSCRIPTION_KEY"
fallback_threshold = 0.70
cost_per_1000 = 1.0
# Features optimized for herbarium specimens
features = ["read", "ocr"]
detect_handwriting = true

[textract]
# AWS Textract - document analysis focused
region = "us-east-1"
fallback_threshold = 0.75
cost_per_1000 = 1.5
# Use document analysis for complex layouts
analyze_document = true
feature_types = ["TABLES", "FORMS", "SIGNATURES"]

[gemini]
# Google Gemini Vision - latest multimodal AI
model = "gemini-pro-vision"
fallback_threshold = 0.80
cost_per_1000 = 2.5
# Enhanced for scientific content
safety_settings = "block_few"
generation_config = {
    "temperature" = 0.1,
    "top_p" = 0.8,
    "top_k" = 40,
    "max_output_tokens" = 1024
}

[pipeline]
steps = ["image_to_text", "text_to_dwc"]

[preprocess]
pipeline = ["grayscale", "deskew", "binarize", "resize"]
binarize_method = "adaptive"  # "otsu" or "adaptive"
max_dim_px = 4000
contrast_factor = 1.5  # used when "contrast" is in the pipeline

[dwc]
schema = "dwc-abcd"
schema_uri = "http://rs.tdwg.org/dwc/terms/"
schema_files = ["dwc.xsd", "abcd.xsd"]
# Schema source configuration
use_official_schemas = false  # Set to true to fetch from official TDWG sources
preferred_official_schemas = ["dwc_simple", "abcd_206"]  # Schema names to use when fetching official schemas
schema_cache_enabled = true  # Enable caching of downloaded schemas
schema_update_interval_days = 30  # How often to check for schema updates
schema_compatibility_check = true  # Validate terms against target schemas
assume_country_if_missing = "Canada"
strict_minimal_fields = ["catalogNumber","scientificName","eventDate","recordedBy","locality"]
flag_if_missing_minimal = true
parse_scientific_names = true
normalize_spelling = true
do_not_update_nomenclature = true

[dwc.custom]
# Add custom field mappings. Example:
# barcode = "catalogNumber"  # raw_field = "dwc_term"

[qc]
dupes = ["catalog","sha256","phash"]
phash_threshold = 10
low_confidence_flag = true
top_fifth_scan_pct = 20

[qc.gbif]
enabled = false
# Primary GBIF API endpoints
species_match_endpoint = "https://api.gbif.org/v1/species/match"
reverse_geocode_endpoint = "https://api.gbif.org/v1/geocode/reverse"
occurrence_search_endpoint = "https://api.gbif.org/v1/occurrence/search"
suggest_endpoint = "https://api.gbif.org/v1/species/suggest"

# Network and retry configuration
timeout = 10.0
retry_attempts = 3
backoff_factor = 1.0
cache_size = 1000

# Verification behavior settings
enable_fuzzy_matching = true
min_confidence_score = 0.80
enable_occurrence_validation = false

# Quality control thresholds
max_coordinate_distance_km = 10.0
min_occurrence_count_threshold = 1

[report]
html = true

[processing]
retry_limit = 3

[export]
# Darwin Core Archive export configuration
enable_versioned_exports = true
default_export_version = "1.0.0"
bundle_format = "rich"  # "rich" (with metadata) or "simple" (version only)
include_checksums = true
include_git_info = true
include_system_info = true
auto_increment_version = false  # Automatically increment patch version
export_retention_days = 365  # How long to keep old exports (0 = keep forever)

# Additional files to include in archives (relative to output directory)
additional_files = []  # Example: ["README.txt", "processing_log.txt"]

# Export filename patterns
rich_filename_pattern = "dwca_{version}_{timestamp}_{commit}_{filter_hash}.zip"
simple_filename_pattern = "dwca_v{version}.zip"
